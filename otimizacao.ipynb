{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criando a Spark Session\n",
    "spark = SparkSession.builder.appName(\"VideoCommentsJoin\").getOrCreate()\n",
    "\n",
    "# Passo 1: Ler os arquivos Parquet\n",
    "# Carregando os dados de vídeos\n",
    "df_video = spark.read.parquet(\"videos-preparados.snappy.parquet\")\n",
    "# Carregando os dados de comentários\n",
    "df_comments = spark.read.parquet(\"video-comments-tratados.snappy.parquet\")\n",
    "\n",
    "# Passo 2: Criar tabelas temporárias\n",
    "# Criando tabelas temporárias para SQL\n",
    "df_video.createOrReplaceTempView(\"videos\")\n",
    "df_comments.createOrReplaceTempView(\"comments\")\n",
    "\n",
    "# Passo 3: Realizar o JOIN utilizando SQL\n",
    "join_video_comments = spark.sql(\"\"\"\n",
    "    SELECT v.*, c.*\n",
    "    FROM videos v\n",
    "    INNER JOIN comments c\n",
    "    ON v.video_id = c.video_id\n",
    "\"\"\")\n",
    "\n",
    "# Passo 4: Repetindo os passos com repartition e coalesce\n",
    "# Reparticionando os dataframes para otimizar a distribuição de dados\n",
    "df_video_repartitioned = df_video.repartition(10)\n",
    "df_comments_repartitioned = df_comments.repartition(10)\n",
    "\n",
    "# Criando tabelas temporárias novamente\n",
    "df_video_repartitioned.createOrReplaceTempView(\"videos_rep\")\n",
    "df_comments_repartitioned.createOrReplaceTempView(\"comments_rep\")\n",
    "\n",
    "# Fazendo o JOIN novamente\n",
    "join_video_comments_rep = spark.sql(\"\"\"\n",
    "    SELECT v.*, c.*\n",
    "    FROM videos_rep v\n",
    "    INNER JOIN comments_rep c\n",
    "    ON v.video_id = c.video_id\n",
    "\"\"\")\n",
    "\n",
    "# Passo 5: Utilizando explain para entender a diferença entre as estratégias\n",
    "print(\"Explicação do JOIN normal:\")\n",
    "join_video_comments.explain(True)\n",
    "print(\"Explicação do JOIN com repartition:\")\n",
    "join_video_comments_rep.explain(True)\n",
    "\n",
    "# Passo 6: Realizando o JOIN de forma otimizada\n",
    "# Utilizando coalesce para otimizar o número de partições antes de criar tabelas\n",
    "df_video_optimized = df_video.coalesce(5)\n",
    "df_comments_optimized = df_comments.coalesce(5)\n",
    "\n",
    "# Criando tabelas temporárias otimizadas\n",
    "df_video_optimized.createOrReplaceTempView(\"videos_opt\")\n",
    "df_comments_optimized.createOrReplaceTempView(\"comments_opt\")\n",
    "\n",
    "# Realizando o JOIN otimizado filtrando apenas colunas necessárias\n",
    "join_video_comments_opt = spark.sql(\"\"\"\n",
    "    SELECT v.video_id, v.title, v.category, c.comment_text, c.user_id, c.like_count\n",
    "    FROM videos_opt v\n",
    "    INNER JOIN comments_opt c\n",
    "    ON v.video_id = c.video_id\n",
    "\"\"\")\n",
    "\n",
    "# Passo 7: Salvando o resultado final no formato Parquet\n",
    "join_video_comments_opt.write.mode(\"overwrite\").parquet(\"join-videos-comments-otimizado\")\n",
    "\n",
    "# Comentários sobre as otimizações:\n",
    "# 1. Utilizamos `repartition(10)` para distribuir os dados igualmente entre os nós antes do JOIN.\n",
    "# 2. Utilizamos `coalesce(5)` para reduzir o número de partições antes do processamento final, otimizando a escrita dos dados.\n",
    "# 3. Reduzimos as colunas selecionadas no JOIN para evitar leitura e processamento desnecessários.\n",
    "# 4. `explain(True)` foi usado para analisar o plano de execução e identificar melhorias na estratégia de processamento.\n",
    "\n",
    "print(\"Processo concluído com otimização!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
